================================================================
Repopack Output File
================================================================

This file was generated by Repopack on: 2024-08-27T12:20:00.594Z

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This header section
2. Repository structure
3. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
1. This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
2. When processing this file, use the separators and "File:" markers to
  distinguish between different files in the repository.
3. Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repopack's
  configuration.
- Binary files are not included in this packed representation. Please refer to
  the Repository Structure section for a complete list of file paths, including
  binary files.



For more information about Repopack, visit: https://github.com/yamadashy/repopack

================================================================
Repository Structure
================================================================
llmx/
  configs/
    config.default.yml
  generators/
    text/
      __init__.py
      anthropic_textgen.py
      base_textgen.py
      cohere_textgen.py
      hf_textgen.py
      openai_textgen.py
      palm_textgen.py
      providers.py
      textgen.py
    __init__.py
  __init__.py
  cli.py
  datamodel.py
  utils.py
  version.py
notebooks/
  research/
    travelbenchmark.ipynb
  tutorial.ipynb
tests/
  test_generators.py
.gitignore
LICENSE
MANIFEST.in
pyproject.toml
README.md
setup.py

================================================================
Repository Files
================================================================

================
File: .gitignore
================
.vscode
.release.sh
llmx/generators/cache
llmx.egg-info
notebooks/test.ipynb
notebooks/data
notebooks/.env
# Byte-compiled / optimized / DLL files
__pycache__/
*.py[cod]
*$py.class
configs/config.yml

.DS_Store
n

# C extensions
*.so

# Distribution / packaging
.Python
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
share/python-wheels/
*.egg-info/
.installed.cfg
*.egg
MANIFEST

# PyInstaller
#  Usually these files are written by a python script from a template
#  before PyInstaller builds the exe, so as to inject date/other infos into it.
*.manifest
*.spec

# Installer logs
pip-log.txt
pip-delete-this-directory.txt

# Unit test / coverage reports
htmlcov/
.tox/
.nox/
.coverage
.coverage.*
.cache
nosetests.xml
coverage.xml
*.cover
*.py,cover
.hypothesis/
.pytest_cache/
cover/

# Translations
*.mo
*.pot

# Django stuff:
*.log
local_settings.py
db.sqlite3
db.sqlite3-journal

# Flask stuff:
instance/
.webassets-cache

# Scrapy stuff:
.scrapy

# Sphinx documentation
docs/_build/

# PyBuilder
.pybuilder/
target/

# Jupyter Notebook
.ipynb_checkpoints

# IPython
profile_default/
ipython_config.py

# pyenv
#   For a library or package, you might want to ignore these files since the code is
#   intended to run in multiple environments; otherwise, check them in:
# .python-version

# pipenv
#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.
#   However, in case of collaboration, if having platform-specific dependencies or dependencies
#   having no cross-platform support, pipenv may install dependencies that don't work, or not
#   install all needed dependencies.
#Pipfile.lock

# poetry
#   Similar to Pipfile.lock, it is generally recommended to include poetry.lock in version control.
#   This is especially recommended for binary packages to ensure reproducibility, and is more
#   commonly ignored for libraries.
#   https://python-poetry.org/docs/basic-usage/#commit-your-poetrylock-file-to-version-control
#poetry.lock

# pdm
#   Similar to Pipfile.lock, it is generally recommended to include pdm.lock in version control.
#pdm.lock
#   pdm stores project-wide configurations in .pdm.toml, but it is recommended to not include it
#   in version control.
#   https://pdm.fming.dev/#use-with-ide
.pdm.toml

# PEP 582; used by e.g. github.com/David-OConnor/pyflow and github.com/pdm-project/pdm
__pypackages__/

# Celery stuff
celerybeat-schedule
celerybeat.pid

# SageMath parsed files
*.sage.py

# Environments
.env
.venv
env/
venv/
ENV/
env.bak/
venv.bak/

# Spyder project settings
.spyderproject
.spyproject

# Rope project settings
.ropeproject

# mkdocs documentation
/site

# mypy
.mypy_cache/
.dmypy.json
dmypy.json

# Pyre type checker
.pyre/

# pytype static type analyzer
.pytype/

# Cython debug symbols
cython_debug/

# PyCharm
#  JetBrains specific template is maintained in a separate JetBrains.gitignore that can
#  be found at https://github.com/github/gitignore/blob/main/Global/JetBrains.gitignore
#  and can be added to the global gitignore or merged into this file.  For a more nuclear
#  option (not recommended) you can uncomment the following to ignore the entire idea folder.
#.idea/

================
File: LICENSE
================
The MIT License (MIT)

Copyright (c) 2023 Victor Dibia.

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in
all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
THE SOFTWARE.

================
File: MANIFEST.in
================
recursive-exclude notebooks
recursive-exclude configs 
recursive-exclude tests

================
File: README.md
================
# LLMX - An API for Chat Fine-Tuned Language Models

[![PyPI version](https://badge.fury.io/py/llmx.svg)](https://badge.fury.io/py/llmx)

A simple python package that provides a unified interface to several LLM providers of chat fine-tuned models [OpenAI, AzureOpenAI, PaLM, Cohere and local HuggingFace Models].

> **Note**
> llmx wraps multiple api providers and its interface _may_ change as the providers as well as the general field of LLMs evolve.

There is nothing particularly special about this library, but some of the requirements I needed when I started building this (that other libraries did not have):

- **Unified Model Interface**: Single interface to create LLM text generators with support for **multiple LLM providers**.

```python
from llmx import  llm

gen = llm(provider="openai") # support azureopenai models too.
gen = llm(provider="palm") # or google
gen = llm(provider="cohere") # or palm
gen = llm(provider="hf", model="HuggingFaceH4/zephyr-7b-beta", device_map="auto") # run huggingface model locally
```

- **Unified Messaging Interface**. Standardizes on the OpenAI ChatML message format and is designed for _chat finetuned_ models. For example, the standard prompt sent a model is formatted as an array of objects, where each object has a role (`system`, `user`, or `assistant`) and content (see below). A single request is list of only one message (e.g., write code to plot a cosine wave signal). A conversation is a list of messages e.g. write code for x, update the axis to y, etc. Same format for all models.

```python
messages = [
    {"role": "user", "content": "You are a helpful assistant that can explain concepts clearly to a 6 year old child."},
    {"role": "user", "content": "What is  gravity?"}
]
```

- **Good Utils (e.g., Caching etc)**: E.g. being able to use caching for faster responses. General policy is that cache is used if config (including messages) is the same. If you want to force a new response, set `use_cache=False` in the `generate` call.

```python
response = gen.generate(messages=messages, config=TextGeneratorConfig(n=1, use_cache=True))
```

Output looks like

```bash

TextGenerationResponse(
  text=[Message(role='assistant', content="Gravity is like a magical force that pulls things towards each other. It's what keeps us on the ground and stops us from floating away into space. ... ")],
  config=TextGenerationConfig(n=1, temperature=0.1, max_tokens=8147, top_p=1.0, top_k=50, frequency_penalty=0.0, presence_penalty=0.0, provider='openai', model='gpt-4', stop=None),
  logprobs=[], usage={'prompt_tokens': 34, 'completion_tokens': 69, 'total_tokens': 103})

```

Are there other libraries that do things like this really well? Yes! I'd recommend looking at [guidance](https://github.com/microsoft/guidance) which does a lot more. Interested in optimized inference? Try somthing like [vllm](https://github.com/vllm-project/vllm).

## Installation

Install from pypi. Please use **python3.10** or higher.

```bash
pip install llmx
```

Install in development mode

```bash
git clone
cd llmx
pip install -e .
```

Note that you may want to use the latest version of pip to install this package.
`python3 -m pip install --upgrade pip`

## Usage

Set your api keys first for each service.

```bash
# for openai and cohere
export OPENAI_API_KEY=<your key>
export COHERE_API_KEY=<your key>

# for PALM via MakerSuite
export PALM_API_KEY=<your key>

# for PaLM (Vertex AI), setup a gcp project, and get a service account key file
export PALM_SERVICE_ACCOUNT_KEY_FILE= <path to your service account key file>
export PALM_PROJECT_ID=<your gcp project id>
export PALM_PROJECT_LOCATION=<your project location>
```

You can also set the default provider and list of supported providers via a config file. Use the yaml format in this [sample `config.default.yml` file](llmx/configs/config.default.yml) and set the `LLMX_CONFIG_PATH` to the path of the config file.

```python
from llmx import llm
from llmx.datamodel import TextGenerationConfig

messages = [
    {"role": "system", "content": "You are a helpful assistant that can explain concepts clearly to a 6 year old child."},
    {"role": "user", "content": "What is  gravity?"}
]

openai_gen = llm(provider="openai")
openai_config = TextGenerationConfig(model="gpt-4", max_tokens=50)
openai_response = openai_gen.generate(messages, config=openai_config, use_cache=True)
print(openai_response.text[0].content)

```

See the [tutorial](/notebooks/tutorial.ipynb) for more examples.

## A Note on Using Local HuggingFace Models

While llmx can use the huggingface transformers library to run inference with local models, you might get more mileage from using a well-optimized server endpoint like [vllm](https://vllm.readthedocs.io/en/latest/getting_started/quickstart.html#openai-compatible-server), or FastChat. The general idea is that these tools let you provide an openai-compatible endpoint but also implement optimizations such as dynamic batching, quantization etc to improve throughput. The general steps are:

- install vllm, setup endpoint e.g., on port `8000`
- use openai as your provider to access that endpoint.

```python
from llmx import  llm
hfgen_gen = llm(
    provider="openai",
    api_base="http://localhost:8000",
    api_key="EMPTY,
)
...
```

## Current Work

- Supported models
  - [x] OpenAI
  - [x] PaLM ([MakerSuite](https://developers.generativeai.google/api/rest/generativelanguage), [Vertex AI](https://cloud.google.com/vertex-ai/docs/generative-ai/learn/models))
  - [x] Cohere
  - [x] HuggingFace (local)

## Caveats

- **Prompting**. llmx makes some assumptions around how prompts are constructed e.g., how the chat message interface is assembled into a prompt for each model type. If your application or use case requires more control over the prompt, you may want to use a different library (ideally query the LLM models directly).
- **Inference Optimization**. For hosted models (GPT-4, PalM, Cohere) etc, this library provides an excellent unified interface as the hosted api already takes care of inference optimizations. However, if you are looking for a library that is optimized for inference with **_local models_(e.g., huggingface)** (tensor parrelization, distributed inference etc), I'd recommend looking at [vllm](https://github.com/vllm-project/vllm) or [tgi](https://github.com/huggingface/text-generation-inference).

## Citation

If you use this library in your work, please cite:

```bibtex
@software{victordibiallmx,
author = {Victor Dibia},
license = {MIT},
month =  {10},
title = {LLMX - An API for Chat Fine-Tuned Language Models},
url = {https://github.com/victordibia/llmx},
year = {2023}
}
```

================
File: pyproject.toml
================
[build-system]
requires = ["setuptools", "setuptools-scm"]
build-backend = "setuptools.build_meta"

[project]
name = "llmx" 
authors = [
  { name="Victor Dibia", email="victor.dibia@gmail.com" },
]
description = "LLMX: A library for LLM Text Generation"
readme = "README.md"
license = { file="LICENSE" }
requires-python = ">=3.9"
classifiers = [
    "Programming Language :: Python :: 3",
    "License :: OSI Approved :: MIT License",
    "Operating System :: OS Independent",
]


dependencies = [
    "pydantic",
    "openai",
    "tiktoken",
    "diskcache",
    "cohere", 
    "google.auth",
    "anthropic",
    "typer",
    "pyyaml",
]
optional-dependencies = {web = ["fastapi", "uvicorn"], transformers = ["transformers[torch]>=4.26","accelerate", "bitsandbytes"]}

dynamic = ["version"]

[tool.setuptools]
include-package-data = true


[tool.setuptools.dynamic]
version = {attr = "llmx.version.VERSION"}
readme = {file = ["README.md"]}

[tool.setuptools.packages.find]  
include = ["llmx*"]  
exclude = ["*.tests*"]  
namespaces = false  

[tool.setuptools.package-data]
"llmx" = ["*.*"]

[tool.pytest.ini_options]
filterwarnings = [
    "ignore:Deprecated call to `pkg_resources\\.declare_namespace\\('.*'\\):DeprecationWarning",
    "ignore::DeprecationWarning:google.rpc",
]


[project.urls]
"Homepage" = "https://github.com/victordibia/llmx"
"Bug Tracker" = "https://github.com/victordibia/llmx/issues"

[project.scripts]
llmx = "llmx.cli:run"

================
File: setup.py
================
from setuptools import setup
setup()

================
File: notebooks/tutorial.ipynb
================
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llmx import  llm, TextGenerationConfig\n",
    "import os   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = TextGenerationConfig( \n",
    "    n=1,\n",
    "    temperature=0.8,\n",
    "    max_tokens=100,\n",
    "    top_p=1.0,\n",
    "    top_k=50,\n",
    "    frequency_penalty=0.0,\n",
    "    presence_penalty=0.0,\n",
    ")\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant that can explain concepts clearly to a 6 year old child.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is  gravity?\"}\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## llmx Supports Multiple Providers \n",
    "\n",
    "### OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gravity is like a big invisible force that pulls things towards each other. It's what keeps us on the ground and makes things fall down instead of floating away. Imagine if you threw a ball up in the air, gravity would pull it back down to the ground. It's like a super strong magnet that pulls everything together.\n"
     ]
    }
   ],
   "source": [
    "openai_gen = llm(provider=\"openai\", api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "openai_config = TextGenerationConfig(model=\"gpt-3.5-turbo\", use_cache=True)\n",
    "openai_response = openai_gen.generate(messages, config=openai_config)\n",
    "print(openai_response.text[0].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Azure OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gravity is like a big invisible force that pulls things towards each other. It's what keeps us on the ground and makes things fall down when we drop them. It's like a big hug from the Earth that keeps us close to it.\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv \n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "azure_openai_gen = llm(\n",
    "    provider=\"openai\",\n",
    "    api_type=\"azure\",\n",
    "    azure_endpoint=os.environ[\"AZURE_OPENAI_BASE\"],\n",
    "    api_key=os.environ[\"AZURE_OPENAI_API_KEY\"],\n",
    "    api_version=\"2023-07-01-preview\",\n",
    ")\n",
    "openai_config = TextGenerationConfig(model=\"gpt-35-turbo-0613\", use_cache=True)\n",
    "openai_response = azure_openai_gen.generate(messages, config=openai_config)\n",
    "print(openai_response.text[0].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PaLM (Google) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PaLM: MakerSuite API \n",
    "\n",
    "- Visit [https://makersuite.google.com/](https://makersuite.google.com/) to get an api key. \n",
    "- Also note that the list of supported models might vary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gravity is a force that pulls objects towards each other. The more massive an object is, the stronger its gravitational pull. The Earth is very massive, so it has a strong gravitational pull. This is why we don't float off into space. The Moon is also massive, but it is much smaller than the Earth. This means that its gravitational pull is not as strong. This is why the Moon orbits the Earth, instead of the other way around.\n",
      "\n",
      "Gravity is a very important force in the universe. It is what keeps the planets in orbit around the Sun, and it is what keeps the Moon in orbit around the Earth. It is also what keeps us on the ground. Without gravity, we would all float off into space.\n",
      "\n",
      "Gravity is a very mysterious force. We don't really know what causes it. We do know that it is related to mass, but we don't know exactly how. Scientists are still working on trying to understand gravity.\n",
      "\n",
      "One way to think about gravity is to imagine a trampoline. If you put a bowling ball in the middle of the trampoline, it will make a dent in the trampoline. If you then put a marble on the trampoline, the marble will roll towards the bowling ball. This is because the bowling ball is more massive than the marble, and it has a stronger gravitational pull.\n",
      "\n",
      "The Earth is like the bowling ball, and we are like the marble. The Earth's gravity pulls us towards the center of the Earth, which is why we don't float off into space.\n",
      "\n",
      "Gravity is a very important force in the universe. It is what keeps the planets in orbit around the Sun, and it is what keeps the Moon in orbit around the Earth. It is also what keeps us on the ground. Without gravity, we would all float off into space.\n"
     ]
    }
   ],
   "source": [
    "palm_gen = llm(\n",
    "    provider=\"palm\",\n",
    "    api_key=os.environ[\"PALM_API_KEY\"],\n",
    ")\n",
    "palm_config = TextGenerationConfig(\n",
    "    model=\"chat-bison-001\", temperature=0, use_cache=True\n",
    ")\n",
    "palm_response = palm_gen.generate(messages, config=palm_config)\n",
    "print(palm_response.text[0].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PaLM: Vertex AI\n",
    "Uses the same API as Google Cloud AI Platform. You will need to setup a service account and download the key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Gravity is a force that pulls objects towards each other. It is what keeps us on the ground and keeps the planets in orbit around the sun. Gravity is always pulling on us, but we don't notice it because we are used to it. But if you jump up in the air, you will feel the force of gravity pulling you back down to the ground.\n"
     ]
    }
   ],
   "source": [
    "palm_gen = llm(\n",
    "    provider=\"palm\",\n",
    "    palm_key_file=os.environ[\"PALM_SERVICE_ACCOUNT_KEY_FILE\"],\n",
    "    project_id=os.environ[\"PALM_PROJECT_ID\"],\n",
    "    project_location=os.environ[\"PALM_PROJECT_LOCATION\"],\n",
    "    api_key=None\n",
    ")\n",
    "palm_config = TextGenerationConfig(\n",
    "    model=\"codechat-bison\", temperature=0, use_cache=True\n",
    ")\n",
    "palm_response = palm_gen.generate(messages, config=palm_config)\n",
    "print(palm_response.text[0].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cohere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gravity is a force that pulls things together. It is what makes things fall to the ground and what holds us on the earth. Gravity is a fundamental force of nature that affects everything around us. It is a property of all matter, and it is what makes things heavy. Gravity is also what causes the moon to orbit the earth and the planets to orbit the sun. It is a very important force that plays a big role in our lives.\n"
     ]
    }
   ],
   "source": [
    "cohere_gen = llm(provider=\"cohere\")\n",
    "cohere_config = TextGenerationConfig(model=\"command\", max_tokens=4050, use_cache=True)\n",
    "cohere_response = cohere_gen.generate(messages, config=cohere_config)\n",
    "print(cohere_response.text[0].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local HuggingFace Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/victordibia/miniconda3/envs/llmx/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/victordibia/miniconda3/envs/llmx/lib/python3.11/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|██████████| 8/8 [00:06<00:00,  1.27it/s]\n"
     ]
    }
   ],
   "source": [
    "hf_generator = llm(provider=\"hf\", model=\"HuggingFaceH4/zephyr-7b-beta\", device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gravity is a special kind of force that pulls things down towards the ground. It's what makes apples fall from trees and why we don't float away into space! Gravity is also what keeps the Earth spinning around and around, so we don't fall off! It's a very strong force that we can't see, but we can feel it pulling us down when we jump into the air. Gravity is a very important force that helps keep everything in the universe in its place!\n"
     ]
    }
   ],
   "source": [
    "hf_config = TextGenerationConfig(temperature=0, max_tokens=650, use_cache=False)\n",
    "hf_response = hf_generator.generate(messages, config=hf_config)\n",
    "print(hf_response.text[0].content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

================
File: tests/test_generators.py
================
import pytest
import os
from llmx import llm
from llmx.datamodel import TextGenerationConfig


config = TextGenerationConfig(
    n=2,
    temperature=0.4,
    max_tokens=100,
    top_p=1.0,
    top_k=50,
    frequency_penalty=0.0,
    presence_penalty=0.0,
    use_cache=False
)

messages = [
    {"role": "user",
     "content": "What is the capital of France? Only respond with the exact answer"}]

def test_anthropic():
    anthropic_gen = llm(provider="anthropic", api_key=os.environ.get("ANTHROPIC_API_KEY", None))
    config.model = "claude-3-5-sonnet-20240620"  # or any other Anthropic model you want to test
    anthropic_response = anthropic_gen.generate(messages, config=config)
    answer = anthropic_response.text[0].content
    print(anthropic_response.text[0].content)

    assert ("paris" in answer.lower())
    assert len(anthropic_response.text) == 1 
    
def test_openai():
    openai_gen = llm(provider="openai")
    openai_response = openai_gen.generate(messages, config=config)
    answer = openai_response.text[0].content
    print(openai_response.text[0].content)

    assert ("paris" in answer.lower())
    assert len(openai_response.text) == 2


def test_google():
    google_gen = llm(provider="palm", api_key=os.environ.get("PALM_API_KEY", None))
    config.model = "chat-bison-001"
    google_response = google_gen.generate(messages, config=config)
    answer = google_response.text[0].content
    print(google_response.text[0].content)

    assert ("paris" in answer.lower())
    # assert len(google_response.text) == 2 palm may chose to return 1 or 2 responses


def test_cohere():
    cohere_gen = llm(provider="cohere")
    config.model = "command"
    cohere_response = cohere_gen.generate(messages, config=config)
    answer = cohere_response.text[0].content
    print(cohere_response.text[0].content)

    assert ("paris" in answer.lower())
    assert len(cohere_response.text) == 2


@pytest.mark.skipif(os.environ.get("LLMX_RUNALL", None) is None
                    or os.environ.get("LLMX_RUNALL", None) == "False", reason="takes too long")
def test_hf_local():
    hf_local_gen = llm(
        provider="hf",
        model="TheBloke/Llama-2-7b-chat-fp16",
        device_map="auto")
    hf_local_response = hf_local_gen.generate(messages, config=config)
    answer = hf_local_response.text[0].content
    print(hf_local_response.text[0].content)

    assert ("paris" in answer.lower())
    assert len(hf_local_response.text) == 2

================
File: llmx/__init__.py
================
import sys
from .generators.text.textgen import llm
from .datamodel import TextGenerationConfig, TextGenerationResponse, Message
from .generators.text.base_textgen import TextGenerator
from .generators.text.providers import providers

if sys.version_info < (3, 9):
    raise RuntimeError("llmx requires Python 3.10+")

================
File: llmx/cli.py
================
import typer
from .generators.text.providers import providers

app = typer.Typer()


@app.command()
def models():
    print("Available models:")
    for provider in providers.items():
        print(f"Provider: {provider[1]['name']}")
        for model in provider[1]["models"]:
            print(f"  - {model['name']}")


@app.command()
def list():
    print("list")


def run():
    app()


if __name__ == "__main__":
    app()

================
File: llmx/datamodel.py
================
from dataclasses import asdict
from typing import Any, Optional, Union, List
from pydantic.dataclasses import dataclass


@dataclass
class Message:
    role: str
    content: str

    def __post_init__(self):
        self._fields_dict = asdict(self)

    def __getitem__(self, key: Union[str, int]) -> Any:
        return self._fields_dict.get(key)

    def to_dict(self):
        return self._fields_dict

    def __iter__(self):
        for key, value in self._fields_dict.items():
            yield key, value


@dataclass
class TextGenerationConfig:
    n: int = 1
    temperature: float = 0.1
    max_tokens: Union[int, None] = None
    top_p: float = 1.0
    top_k: int = 50
    frequency_penalty: float = 0.0
    presence_penalty: float = 0.0
    provider: Union[str, None] = None
    model: Optional[str] = None
    stop: Union[List[str], str, None] = None
    use_cache: bool = True

    def __post_init__(self):
        self._fields_dict = asdict(self)

    def __getitem__(self, key: Union[str, int]) -> Any:
        return self._fields_dict.get(key)

    def __iter__(self):
        for key, value in self._fields_dict.items():
            yield key, value


@dataclass
class TextGenerationResponse:
    """Response from a text generation"""

    text: List[Message]
    config: Any
    logprobs: Optional[Any] = None  # logprobs if available
    usage: Optional[Any] = None  # usage statistics from the provider
    response: Optional[Any] = None  # full response from the provider

    def __post_init__(self):
        self._fields_dict = asdict(self)

    def __getitem__(self, key: Union[str, int]) -> Any:
        return self._fields_dict.get(key)

    def __iter__(self):
        for key, value in self._fields_dict.items():
            yield key, value

    def to_dict(self):
        return self._fields_dict

    def __json__(self):
        return self._fields_dict

================
File: llmx/utils.py
================
from dataclasses import asdict
import logging
import json
from typing import Any, Union, Dict
import tiktoken
from diskcache import Cache
import hashlib
import os
import platform
import google.auth
import google.auth.transport.requests
from google.oauth2 import service_account
import requests
import yaml

logger = logging.getLogger("llmx")


def num_tokens_from_messages(messages, model="gpt-3.5-turbo-0301"):
    """Returns the number of tokens used by a list of messages."""
    try:
        encoding = tiktoken.encoding_for_model(model)
    except KeyError:
        encoding = tiktoken.get_encoding("cl100k_base")
    if (
        model == "gpt-3.5-turbo-0301" or True
    ):  # note: future models may deviate from this
        num_tokens = 0
        for message in messages:
            if not isinstance(message, dict):
                message = asdict(message)

            num_tokens += (
                4  # every message follows <im_start>{role/name}\n{content}<im_end>\n
            )

            for key, value in message.items():
                num_tokens += len(encoding.encode(value))
                if key == "name":  # if there's a name, the role is omitted
                    num_tokens += -1  # role is always required and always 1 token
        num_tokens += 2  # every reply is primed with <im_start>assistant
        return num_tokens


def cache_request(cache: Cache, params: dict, values: Union[Dict, None] = None) -> Any:
    # Generate a unique key for the request

    key = hashlib.md5(json.dumps(params, sort_keys=True).encode("utf-8")).hexdigest()
    # Check if the request is cached
    if key in cache and values is None:
        # print("retrieving from cache")
        return cache[key]

    # Cache the provided values and return them
    if values:
        # print("saving to cache")
        cache[key] = values
    return values


def get_user_cache_dir(app_name: str) -> str:
    system = platform.system()
    if system == "Windows":
        cache_path = os.path.join(os.getenv("LOCALAPPDATA"), app_name, "Cache")
    elif system == "Darwin":
        cache_path = os.path.join(os.path.expanduser("~/Library/Caches"), app_name)
    else:  # Linux and other UNIX-like systems
        cache_path = os.path.join(
            os.getenv("XDG_CACHE_HOME", os.path.expanduser("~/.cache")), app_name
        )
    os.makedirs(cache_path, exist_ok=True)
    return cache_path


def get_gcp_credentials(service_account_key_file: str = None, scopes: list[str] = [
        'https://www.googleapis.com/auth/cloud-platform']):
    try:
        # Attempt to use Application Default Credentials
        credentials, project_id = google.auth.default(scopes=scopes)
        auth_req = google.auth.transport.requests.Request()
        credentials.refresh(auth_req)
        return credentials
    except google.auth.exceptions.DefaultCredentialsError:
        # Fall back to using service account key
        if service_account_key_file is None:
            raise ValueError(
                "Service account key file is not set. Please set the PALM_SERVICE_ACCOUNT_KEY_FILE environment variable."
            )
        credentials = service_account.Credentials.from_service_account_file(
            service_account_key_file, scopes=scopes)
        auth_req = google.auth.transport.requests.Request()
        credentials.refresh(auth_req)
        return credentials


def gcp_request(
    url: str,
    method: str = "POST",
    body: dict = None,
    headers: dict = None,
    credentials: google.auth.credentials.Credentials = None,
    request_timeout: int = 60,
    **kwargs,
):

    headers = headers or {}

    if "key" not in url:
        if credentials is None:
            credentials = get_gcp_credentials()
        auth_req = google.auth.transport.requests.Request()
        if credentials.expired:
            credentials.refresh(auth_req)
        headers["Authorization"] = f"Bearer {credentials.token}"
    headers["Content-Type"] = "application/json"

    response = requests.request(
        method=method, url=url, json=body, headers=headers, timeout=request_timeout, **kwargs
    )

    if response.status_code not in range(200, 300):
        try:
            error_message = response.json().get("error", {}).get("message", "")
        except json.JSONDecodeError:
            error_message = response.content
        raise Exception(
            f"Request failed with status code {response.status_code}: {error_message}"
        )

    return response.json()


def load_config():
    try:
        config_path = os.environ.get("LLMX_CONFIG_PATH", None)
        if config_path is None or os.path.exists(config_path) is False:
            config_path = os.path.join(
                os.path.dirname(__file__),
                "configs/config.default.yml")
            logger.info(
                "Info: LLMX_CONFIG_PATH environment variable is not set to a valid config file. Using default config file at '%s'.",
                config_path)
        if config_path is not None:
            try:
                with open(config_path, "r", encoding="utf-8") as f:
                    config = yaml.safe_load(f)
                    logger.info(
                        "Loaded config from '%s'.",
                        config_path)
                    return config
            except FileNotFoundError as file_not_found:
                logger.info(
                    "Error: Config file not found at '%s'. Please check the LLMX_CONFIG_PATH environment variable. %s",
                    config_path,
                    str(file_not_found))
            except IOError as io_error:
                logger.info(
                    "Error: Could not read the config file at '%s'. %s",
                    config_path, str(io_error))
            except yaml.YAMLError as yaml_error:
                logger.info(
                    "Error: Malformed YAML in config file at '%s'. %s",
                    config_path, str(yaml_error))
        else:
            logger.info(
                "Info:LLMX_CONFIG_PATH environment variable is not set. Please set it to the path of your config file to setup your default model.")
    except Exception as error:
        logger.info("Error: An unexpected error occurred: %s", str(error))

    return None


def get_models_maxtoken_dict(models_list):
    if not models_list:
        return {}

    models_dict = {}
    for model in models_list:
        if "model" in model and "parameters" in model["model"]:
            details = model["model"]["parameters"]
            models_dict[details["model"]] = model["max_tokens"]
    return models_dict

================
File: llmx/version.py
================
VERSION = "0.0.21a"
APP_NAME = "llmx"

================
File: notebooks/research/travelbenchmark.ipynb
================
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llmx.generators.text.textgen import TextGenerator\n",
    "from llmx.datamodel import TextGenerationConfig \n",
    "\n",
    "config = TextGenerationConfig( \n",
    "    n=1,\n",
    "    temperature=0.0,\n",
    "    max_tokens=100,\n",
    "    top_p=1.0,\n",
    "    top_k=50,\n",
    "    frequency_penalty=0.0,\n",
    "    presence_penalty=0.0,\n",
    "    messages =  [\n",
    "        {\"role\": \"user\", \"content\": \"What is the height of the Eiffel Tower?\"},\n",
    "    ]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

================
File: llmx/configs/config.default.yml
================
# Sets the the default model to use for llm() when no provider parameter is set.
model:
  provider: openai
  parameters:
    api_key: null

# list of supported providers.
providers:
  anthropic:
    name: Anthropic
    description: Anthropic's Claude models.
    models:
      - name: claude-3-5-sonnet-20240620
        max_tokens: 8192
        model:
          provider: anthropic
          parameters:
            model: claude-3-5-sonnet-20240620
  openai:
    name: OpenAI
    description: OpenAI's and AzureOpenAI GPT-3 and GPT-4 models.
    models:
      - name: gpt-4o # general model name, can be anything
        max_tokens: 4096 # max supported tokens
        model:
          provider: openai
          parameters:
            model: gpt-4o 
      - name: gpt-4 # general model name, can be anything
        max_tokens: 8192 # max supported tokens
        model:
          provider: openai
          parameters:
            model: gpt-4 # model actual name, required
      - name: gpt-4-32k
        max_tokens: 32768
        model:
          provider: openai
          parameters:
            model: gpt-4-32k
      - name: gpt-3.5-turbo
        max_tokens: 4096
        model:
          provider: openai
          parameters:
            model: gpt-3.5-turbo
      - name: gpt-3.5-turbo-0301
        max_tokens: 4096
        model:
          provider: openai
          parameters:
            model: gpt-3.5-turbo-0301
      - name: gpt-3.5-turbo-16k
        max_tokens: 16384
        model:
          provider: openai
          parameters:
            model: gpt-3.5-turbo-16k
      - name: gpt-3.5-turbo-azure
        max_tokens: 4096
        model:
          provider: azureopenai
          parameters:
            api_key: <your-api-key>
            api_type: azure
            api_base: <your-api-base>
            api_version: <your-api-version>
            organization: <your-organization> # or null
            model: gpt-3.5-turbo-0316
  palm:
    name: Google
    description: Google's LLM models.
    models:
      - name: chat-bison-vertexai
        max_tokens: 1024
        model:
          provider: palm
          parameters:
            model: codechat-bison@001
            project_id: <your-project-id>
            project_location: <your-project-location>
            palm_key_file: <path-to-your-palm-key-file>
      - name: chat-bison-makersuite
        max_tokens: 1024
        model:
          provider: palm
          parameters:
            model: chat-bison-001
            api_key: <your-makersuite-api-key>
      - name: codechat-bison-makersuite
        max_tokens: 1024
        model:
          provider: palm
          parameters:
            model: codechat-bison-001
            api_key: <your-makersuite-api-key>
      - name: codechat-bison-32k
        max_tokens: 32768
        model:
          provider: palm
          parameters:
            model: codechat-bison-32k
            project_id: <your-project-id>
            project_location: <your-project-location>
            palm_key_file: <path-to-your-palm-key-file>
      - name: chat-bison-32k
        max_tokens: 32768
        model:
          provider: palm
          parameters:
            model: codechat-bison-32k
            project_id: <your-project-id>
            project_location: <your-project-location>
            palm_key_file: <path-to-your-palm-key-file>
  cohere:
    name: Cohere
    description: Cohere's LLM models.
    models:
      - name: command
        max_tokens: 4096
        model:
          provider: cohere
          parameters:
            model: command
      - name: command-nightly
        max_tokens: 4096
        model:
          provider: cohere
          parameters:
            model: command-nightly
  huggingface:
    name: HuggingFace
    description: HuggingFace's LLM models.
    models:
      - name: TheBloke/Llama-2-7b-chat-fp16
        max_tokens: 4096
        model:
          provider: huggingface
          parameters:
            model: TheBloke/Llama-2-7b-chat-fp16
            device_map: auto
      - name: hermes-orca-platypus-13b
        max_tokens: 4096
        model:
          provider: huggingface
          parameters:
            model: uukuguy/speechless-llama2-hermes-orca-platypus-13b
            device_map: auto
            trust_remote_code: true

================
File: llmx/generators/__init__.py
================
# from .text.textgen import TextGenerator
from .text.textgen import llm
from .text.base_textgen import TextGenerator

================
File: llmx/generators/text/__init__.py
================
from .textgen import llm

================
File: llmx/generators/text/anthropic_textgen.py
================
from typing import Union, List, Dict
import os
import anthropic
from dataclasses import asdict

from .base_textgen import TextGenerator
from ...datamodel import TextGenerationConfig, TextGenerationResponse, Message
from ...utils import cache_request, get_models_maxtoken_dict, num_tokens_from_messages


class AnthropicTextGenerator(TextGenerator):
    def __init__(
        self,
        api_key: str = None,
        provider: str = "anthropic",
        model: str = None,
        models: Dict = None,
    ):
        super().__init__(provider=provider)
        api_key = api_key or os.environ.get("ANTHROPIC_API_KEY", None)
        if api_key is None:
            raise ValueError(
                "Anthropic API key is not set. Please set the ANTHROPIC_API_KEY environment variable."
            )
        self.client = anthropic.Anthropic(
            api_key=api_key,
            default_headers={"anthropic-beta": "max-tokens-3-5-sonnet-2024-07-15"},
        )
        self.model_max_token_dict = get_models_maxtoken_dict(models)
        self.model_name = model or "claude-3-5-sonnet-20240620"

    def format_messages(self, messages):
        formatted_messages = []
        for message in messages:
            formatted_message = {"role": message["role"], "content": message["content"]}
            formatted_messages.append(formatted_message)
        return formatted_messages


    def generate(
        self,
        messages: Union[List[Dict], str],
        config: TextGenerationConfig = TextGenerationConfig(),
        **kwargs,
    ) -> TextGenerationResponse:
        use_cache = config.use_cache
        model = config.model or self.model_name
        prompt_tokens = num_tokens_from_messages(messages)
        max_tokens = max(
            self.model_max_token_dict.get(model, 8192) - prompt_tokens - 10, 200
        )

        # Process messages
        system_message = None
        other_messages = []
        for message in messages:
            message["content"] = message["content"].strip()
            if message["role"] == "system":
                if system_message is None:
                    system_message = message["content"]
                else:
                    # If multiple system messages, concatenate them
                    system_message += "\n" + message["content"]
            else:
                other_messages.append(message)

        if not other_messages:
            raise ValueError("At least one message is required")

        # Check if inversion is needed
        needs_inversion = other_messages[0]["role"] == "assistant"
        if needs_inversion:
            other_messages = self.invert_messages(other_messages)

        anthropic_config = {
            "model": model,
            "max_tokens": config.max_tokens or max_tokens,
            "temperature": config.temperature,
            "top_p": config.top_p,
            "messages": other_messages,
        }

        if system_message:
            anthropic_config["system"] = system_message

        self.model_name = model
        cache_key_params = anthropic_config.copy()
        cache_key_params["messages"] = messages  # Keep original messages for caching

        if use_cache:
            response = cache_request(cache=self.cache, params=cache_key_params)
            if response:
                return TextGenerationResponse(**response)
        anthropic_response = self.client.messages.create(**anthropic_config)

        response_content = anthropic_response.content[0].text

        # Always strip "Human: " prefix, regardless of inversion
        if response_content.startswith("Human: "):
            response_content = response_content[7:]

        response = TextGenerationResponse(
            text=[Message(role="assistant", content=response_content)],
            logprobs=[],
            config=anthropic_config,
            usage={
                "prompt_tokens": anthropic_response.usage.input_tokens,
                "completion_tokens": anthropic_response.usage.output_tokens,
                "total_tokens": anthropic_response.usage.input_tokens
                + anthropic_response.usage.output_tokens,
            },
            response=anthropic_response,
        )

        cache_request(
            cache=self.cache, params=cache_key_params, values=asdict(response)
        )
        return response

    def invert_messages(self, messages):
        inverted = []
        for i, message in enumerate(messages):
            if i % 2 == 0:
                inverted.append({"role": "user", "content": message["content"]})
            else:
                inverted.append({"role": "assistant", "content": message["content"]})
        return inverted
    def count_tokens(self, text) -> int:
        return num_tokens_from_messages(text)

================
File: llmx/generators/text/base_textgen.py
================
import os
from typing import Union, List, Dict
from diskcache import Cache
from ...utils import get_user_cache_dir
from ...datamodel import TextGenerationConfig, TextGenerationResponse
from ...version import APP_NAME
from abc import ABC, abstractmethod


class TextGenerator(ABC):

    def __init__(self, provider: str = "openai", **kwargs):
        self.provider = provider
        self.model_name = kwargs.get("model_name", "gpt-3.5-turbo")

        app_name = APP_NAME
        cache_dir_default = get_user_cache_dir(app_name)
        cache_dir_based_on_model = os.path.join(cache_dir_default, self.provider, self.model_name)
        self.cache_dir = kwargs.get("cache_dir", cache_dir_based_on_model)
        self.cache = Cache(self.cache_dir)

    @abstractmethod
    def generate(
            self, messages: Union[List[Dict],
                                  str],
            config: TextGenerationConfig = TextGenerationConfig(),
            **kwargs) -> TextGenerationResponse:
        pass

    @abstractmethod
    def count_tokens(self, text) -> int:
        pass

================
File: llmx/generators/text/cohere_textgen.py
================
from typing import Dict, Union
import os
import cohere
from dataclasses import asdict

from .base_textgen import TextGenerator
from ...datamodel import TextGenerationConfig, TextGenerationResponse, Message
from ...utils import cache_request, get_models_maxtoken_dict, num_tokens_from_messages
from ..text.providers import providers


class CohereTextGenerator(TextGenerator):
    def __init__(
        self,
        api_key: str = None,
        provider: str = "cohere",
        model: str = None,
        models: Dict = None,
    ):
        super().__init__(provider=provider)
        api_key = api_key or os.environ.get("COHERE_API_KEY", None)
        if api_key is None:
            raise ValueError(
                "Cohere API key is not set. Please set the COHERE_API_KEY environment variable."
            )
        self.client = cohere.Client(api_key)
        self.model_max_token_dict = get_models_maxtoken_dict(models)
        self.model_name = model or "command"

    def format_messages(self, messages):
        prompt = ""
        for message in messages:
            if message["role"] == "system":
                prompt += message["content"] + "\n"
            else:
                prompt += message["role"] + ": " + message["content"] + "\n"

        return prompt

    def generate(
        self,
        messages: Union[list[dict], str],
        config: TextGenerationConfig = TextGenerationConfig(),
        **kwargs,
    ) -> TextGenerationResponse:
        use_cache = config.use_cache
        messages = self.format_messages(messages)
        self.model_name = config.model or self.model_name

        max_tokens = (
            self.model_max_token_dict[self.model_name]
            if config.model in self.model_max_token_dict else 1024)

        cohere_config = {
            "model": self.model_name,
            "prompt": messages,
            "max_tokens": config.max_tokens or max_tokens,
            "temperature": config.temperature,
            "k": config.top_k,
            "p": config.top_p,
            "num_generations": config.n,
            "stop_sequences": config.stop,
            "frequency_penalty": config.frequency_penalty,
            "presence_penalty": config.presence_penalty,
        }

        # print("calling cohere ***************", config)

        cache_key_params = cohere_config | {"messages": messages}
        if use_cache:
            response = cache_request(cache=self.cache, params=cache_key_params)
            if response:
                return TextGenerationResponse(**response)

        co_response = self.client.generate(**cohere_config)

        response_text = [
            Message(
                role="system",
                content=x.text,
            )
            for x in co_response.generations
        ]

        response = TextGenerationResponse(
            text=response_text,
            logprobs=[],  # You may need to extract log probabilities from the response if needed
            config=cohere_config,
            usage={},
            response=co_response,
        )

        cache_request(
            cache=self.cache, params=cache_key_params, values=asdict(response)
        )
        return response

    def count_tokens(self, text) -> int:
        return num_tokens_from_messages(text)

================
File: llmx/generators/text/hf_textgen.py
================
from typing import Dict, Union
from dataclasses import asdict, dataclass
from transformers import (AutoTokenizer, AutoModelForCausalLM, GenerationConfig, BitsAndBytesConfig)
import torch


from .base_textgen import TextGenerator
from ...datamodel import TextGenerationConfig, TextGenerationResponse
from ...utils import cache_request, get_models_maxtoken_dict


@dataclass
class DialogueTemplate:
    system: str = None
    dialogue_type: str = "default"
    messages: list[dict[str, str]] = None
    system_token: str = "<|system|>"
    user_token: str = "<|user|>"
    assistant_token: str = "<|assistant|>"
    end_token: str = "<|end|>"

    def get_inference_prompt(self) -> str:
        if self.dialogue_type == "default":
            prompt = ""
            system_prompt = (
                self.system_token + "\n" + self.system + self.end_token + "\n"
                if self.system
                else ""
            )
            if self.messages is None:
                raise ValueError("Dialogue template must have at least one message.")
            for message in self.messages:
                if message["role"] == "system":
                    system_prompt += (
                        self.system_token
                        + "\n"
                        + message["content"]
                        + self.end_token
                        + "\n"
                    )
                elif message["role"] == "user":
                    prompt += (
                        self.user_token
                        + "\n"
                        + message["content"]
                        + self.end_token
                        + "\n"
                    )
                else:
                    prompt += (
                        self.assistant_token
                        + "\n"
                        + message["content"]
                        + self.end_token
                        + "\n"
                    )
            prompt += self.assistant_token
            if system_prompt:
                prompt = system_prompt + prompt
            return prompt
        elif self.dialogue_type == "alpaca":
            prompt = (
                self.user_token + "\n" + (self.system + "\n" if self.system else "")
            )
            for message in self.messages:
                prompt += message["content"] + "\n"
            prompt = prompt + " " + self.assistant_token + "\n"
            # print(instruction)
            return prompt
        elif self.dialogue_type == "llama2":
            prompt = "[INST]"
            system_prompt = ""
            other_prompt = ""

            for message in self.messages:
                if message["role"] == "system":
                    system_prompt += message["content"] + "\n"
                elif message["role"] == "assistant":
                    other_prompt += message["content"] + "  \n"
                else:
                    other_prompt += "[INST] " + message["content"] + "[/INST]\n"

            prompt = (
                prompt
                + f" <<SYS>> {system_prompt} <</SYS>> \n"
                + other_prompt
                + "[/INST]"
            )


class HFTextGenerator(TextGenerator):
    def __init__(self,
                 provider: str = "huggingface",
                 models: Dict = None,
                 device_map=None, **kwargs):

        super().__init__(provider=provider)

        self.dialogue_type = kwargs.get("dialogue_type", "alpaca")

        self.model_name = kwargs.get("model", "uukuguy/speechless-llama2-hermes-orca-platypus-13b")
        self.quantization_config = kwargs.get("quantization_config", BitsAndBytesConfig())
        self.trust_remote_code = kwargs.get("trust_remote_code", False)
        self.device = kwargs.get("device", self.get_default_device())

        # load tokenizer and model
        self.tokenizer = AutoTokenizer.from_pretrained(
            self.model_name, trust_remote_code=self.trust_remote_code)
        self.model = AutoModelForCausalLM.from_pretrained(
            self.model_name,
            device_map=device_map,
            quantization_config=self.quantization_config,
            trust_remote_code=self.trust_remote_code,
        )
        if not device_map:
            self.model.to(self.device)
        self.model.config.pad_token_id = self.tokenizer.pad_token_id

        self.max_length = kwargs.get("max_length", 1024)

        self.model_max_token_dict = get_models_maxtoken_dict(models)
        self.max_context_tokens = kwargs.get(
            "max_context_tokens", self.model.config.max_position_embeddings
        ) or self.model_max_token_dict[self.model_name]

        if self.dialogue_type == "alpaca":
            self.dialogue_template = DialogueTemplate(
                dialogue_type="alpaca",
                end_token=self.tokenizer.eos_token,
                user_token="### Instruction:",
                assistant_token="### Response:",
            )
            self.model.config.pad_token_id = self.tokenizer.pad_token_id = 0  # unk
            self.model.config.bos_token_id = 1
            self.model.config.eos_token_id = 2
        else:
            self.dialogue_template = DialogueTemplate(end_token=self.tokenizer.eos_token)

    def get_default_device(self):
        """Pick GPU if available, else CPU"""
        if torch.cuda.is_available():
            return torch.device("cuda")
        elif torch.backends.mps.is_available():
            return torch.device("mps")
        else:
            return torch.device("cpu")

    def post_process_response(self, response):
        response = (
            response.split(self.dialogue_template.assistant_token)[-1]
            .replace(self.dialogue_template.end_token, "")
            .strip()
        )
        response = {"role": "assistant", "content": response}
        return response

    def messages_to_instruction(self, messages):
        instruction = "### Instruction: "
        for message in messages:
            instruction += message["content"] + "\n"
        instruction = instruction + "### Response: "
        # print(instruction)
        return instruction

    def generate(
            self, messages: Union[list[dict],
                                  str],
            config: TextGenerationConfig = TextGenerationConfig(),
            **kwargs) -> TextGenerationResponse:
        use_cache = config.use_cache
        config.model = self.model_name
        cache_key_params = {
            **asdict(config),
            **kwargs,
            "messages": messages,
            "dialogue_type": self.dialogue_type}
        if use_cache:
            response = cache_request(cache=self.cache, params=(cache_key_params))
            if response:
                return TextGenerationResponse(**response)

        self.dialogue_template.messages = messages
        prompt = self.dialogue_template.get_inference_prompt()
        batch = self.tokenizer(
            prompt, return_tensors="pt", return_token_type_ids=False
        ).to(self.model.device)
        input_ids = batch["input_ids"]

        max_new_tokens = kwargs.get(
            "max_new_tokens", self.max_context_tokens - input_ids.shape[-1]
        )
        # print(
        #     "***********Prompt tokens: ",
        #     input_ids.shape[-1],
        #     " | Max new tokens: ",
        #     max_new_tokens,
        # )

        top_k = kwargs.get("top_k", config.top_k)
        min_new_tokens = kwargs.get("min_new_tokens", 32)
        repetition_penalty = kwargs.get("repetition_penalty", 1.0)

        gen_config = GenerationConfig(
            max_new_tokens=max_new_tokens,
            temperature=max(config.temperature, 0.01),
            top_p=config.top_p,
            top_k=top_k,
            num_return_sequences=config.n,
            do_sample=True,
            pad_token_id=self.tokenizer.eos_token_id,
            eos_token_id=self.tokenizer.eos_token_id,
            min_new_tokens=min_new_tokens,
            repetition_penalty=repetition_penalty,
        )
        with torch.no_grad():
            generated_ids = self.model.generate(**batch, generation_config=gen_config)

        text_response = self.tokenizer.batch_decode(
            generated_ids, skip_special_tokens=False
        )

        # print(text_response, "*************")
        prompt_tokens = len(batch["input_ids"][0])
        total_tokens = 0
        for row in generated_ids:
            total_tokens += len(row)

        usage = {
            "prompt_tokens": prompt_tokens,
            "completion_tokens": total_tokens - prompt_tokens,
            "total_tokens": total_tokens,
        }

        response = TextGenerationResponse(
            text=[self.post_process_response(x) for x in text_response],
            logprobs=[],
            config=config,
            usage=usage,
        )
        # if use_cache:
        cache_request(cache=self.cache, params=(cache_key_params), values=asdict(response))
        return response

    def count_tokens(self, text: str):
        return len(self.tokenizer(text)["input_ids"])

================
File: llmx/generators/text/openai_textgen.py
================
from typing import Union, List, Dict
from .base_textgen import TextGenerator
from ...datamodel import Message, TextGenerationConfig, TextGenerationResponse
from ...utils import cache_request, get_models_maxtoken_dict, num_tokens_from_messages
import os
from openai import AzureOpenAI, OpenAI
from dataclasses import asdict


class OpenAITextGenerator(TextGenerator):
    def __init__(
        self,
        api_key: str = os.environ.get("OPENAI_API_KEY", None),
        provider: str = "openai",
        organization: str = None,
        api_type: str = None,
        api_version: str = None,
        azure_endpoint: str = None,
        model: str = None,
        models: Dict = None,
    ):
        super().__init__(provider=provider)
        self.api_key = api_key or os.environ.get("OPENAI_API_KEY", None)

        if self.api_key is None:
            raise ValueError(
                "OpenAI API key is not set. Please set the OPENAI_API_KEY environment variable."
            )

        self.client_args = {
            "api_key": self.api_key,
            "organization": organization,
            "api_version": api_version,
            "azure_endpoint": azure_endpoint,
        }
        # remove keys with None values
        self.client_args = {k: v for k,
                            v in self.client_args.items() if v is not None}

        if api_type:
            if api_type == "azure":
                self.client = AzureOpenAI(**self.client_args)
            else:
                raise ValueError(f"Unknown api_type: {api_type}")
        else:
            self.client = OpenAI(**self.client_args)

        self.model_name = model or "gpt-3.5-turbo"
        self.model_max_token_dict = get_models_maxtoken_dict(models)

    def generate(
        self,
        messages: Union[List[dict], str],
        config: TextGenerationConfig = TextGenerationConfig(),
        **kwargs,
    ) -> TextGenerationResponse:
        use_cache = config.use_cache
        model = config.model or self.model_name
        prompt_tokens = num_tokens_from_messages(messages)
        max_tokens = max(
            self.model_max_token_dict.get(
                model, 4096) - prompt_tokens - 10, 200
        )

        oai_config = {
            "model": model,
            "temperature": config.temperature,
            "max_tokens": max_tokens,
            "top_p": config.top_p,
            "frequency_penalty": config.frequency_penalty,
            "presence_penalty": config.presence_penalty,
            "n": config.n,
            "messages": messages,
        }

        self.model_name = model
        cache_key_params = (oai_config) | {"messages": messages}
        if use_cache:
            response = cache_request(cache=self.cache, params=cache_key_params)
            if response:
                return TextGenerationResponse(**response)

        oai_response = self.client.chat.completions.create(**oai_config)

        response = TextGenerationResponse(
            text=[Message(**x.message.model_dump())
                  for x in oai_response.choices],
            logprobs=[],
            config=oai_config,
            usage=dict(oai_response.usage),
        )
        # if use_cache:
        cache_request(
            cache=self.cache, params=cache_key_params, values=asdict(response)
        )
        return response

    def count_tokens(self, text) -> int:
        return num_tokens_from_messages(text)

================
File: llmx/generators/text/palm_textgen.py
================
from dataclasses import asdict
import os
import logging
from typing import Dict, Union
from .base_textgen import TextGenerator
from ...datamodel import TextGenerationConfig, TextGenerationResponse, Message
from ...utils import (
    cache_request,
    gcp_request,
    get_models_maxtoken_dict,
    num_tokens_from_messages,
    get_gcp_credentials,
)

logger = logging.getLogger("llmx")


class PalmTextGenerator(TextGenerator):
    def __init__(
        self,
        api_key: str = os.environ.get("PALM_API_KEY", None),
        palm_key_file: str = os.environ.get("PALM_SERVICE_ACCOUNT_KEY_FILE", None),
        project_id: str = os.environ.get("PALM_PROJECT_ID", None),
        project_location=os.environ.get("PALM_PROJECT_LOCATION", "us-central1"),
        provider: str = "palm",
        model: str = None,
        models: Dict = None,
    ):
        super().__init__(provider=provider)

        if api_key is None and palm_key_file is None:
            raise ValueError(
                "PALM_API_KEY or PALM_SERVICE_ACCOUNT_KEY_FILE  must be set."
            )
        if api_key:
            self.api_key = api_key
            self.credentials = None
            self.project_id = None
            self.project_location = None
        else:
            self.project_id = project_id
            self.project_location = project_location
            self.api_key = None
            self.credentials = get_gcp_credentials(palm_key_file) if palm_key_file else None

        self.model_max_token_dict = get_models_maxtoken_dict(models)
        self.model_name = model or "chat-bison"

    def format_messages(self, messages):
        palm_messages = []
        system_messages = ""
        for message in messages:
            if message["role"] == "system":
                system_messages += message["content"] + "\n"
            else:
                if not palm_messages or palm_messages[-1]["author"] != message["role"]:
                    palm_message = {
                        "author": message["role"],
                        "content": message["content"],
                    }
                    palm_messages.append(palm_message)
                else:
                    palm_messages[-1]["content"] += "\n" + message["content"]

        if palm_messages and len(palm_messages) % 2 == 0:
            merged_content = (
                palm_messages[-2]["content"] + "\n" + palm_messages[-1]["content"]
            )
            palm_messages[-2]["content"] = merged_content
            palm_messages.pop()

        if len(palm_messages) == 0:
            logger.info("No messages to send to PALM")

        return system_messages, palm_messages

    def generate(
        self,
        messages: Union[list[dict], str],
        config: TextGenerationConfig = TextGenerationConfig(),
        **kwargs,
    ) -> TextGenerationResponse:
        use_cache = config.use_cache
        model = config.model or self.model_name

        system_messages, messages = self.format_messages(messages)
        self.model_name = model

        max_tokens = self.model_max_token_dict[model] if model in self.model_max_token_dict else 1024
        palm_config = {
            "temperature": config.temperature,
            "maxOutputTokens": config.max_tokens or max_tokens,
            "candidateCount": config.n,
        }

        api_url = ""
        if self.api_key:
            api_url = f"https://generativelanguage.googleapis.com/v1beta2/models/{model}:generateMessage?key={self.api_key}"

            palm_payload = {
                "prompt": {"messages": messages, "context": system_messages},
                "temperature": config.temperature,
                "candidateCount": config.n,
                "topP": config.top_p,
                "topK": config.top_k,
            }

        else:
            api_url = f"https://us-central1-aiplatform.googleapis.com/v1/projects/{self.project_id}/locations/{self.project_location}/publishers/google/models/{model}:predict"

            palm_payload = {
                "instances": [
                    {
                        "messages": messages,
                        "context": system_messages,
                        "examples": [],
                    }
                ],
                "parameters": palm_config,
            }

        cache_key_params = {**palm_payload, "model": model, "api_url": api_url}

        if use_cache:
            response = cache_request(cache=self.cache, params=cache_key_params)
            if response:
                return TextGenerationResponse(**response)

        palm_response = gcp_request(
            url=api_url, body=palm_payload, method="POST", credentials=self.credentials
        )

        candidates = palm_response["candidates"] if self.api_key else palm_response["predictions"][
            0]["candidates"]

        response_text = [
            Message(
                role="assistant" if x["author"] == "1" else x["author"],
                content=x["content"],
            )
            for x in candidates
        ]

        response = TextGenerationResponse(
            text=response_text,
            logprobs=[],
            config=palm_config,
            usage={
                "total_tokens": num_tokens_from_messages(
                    response_text, model=self.model_name
                )
            },
            response=palm_response,
        )

        cache_request(
            cache=self.cache, params=(cache_key_params), values=asdict(response)
        )
        return response

    def count_tokens(self, text) -> int:
        return num_tokens_from_messages(text)

================
File: llmx/generators/text/providers.py
================
# This file contains the list of providers and models that are available supported by LLMX.


from llmx.utils import load_config


config = load_config()
providers = providers = config["providers"] if "providers" in config else None

providers = config["providers"]

================
File: llmx/generators/text/textgen.py
================
from ...utils import load_config
from .openai_textgen import OpenAITextGenerator
from .palm_textgen import PalmTextGenerator
from .cohere_textgen import CohereTextGenerator
from .anthropic_textgen import AnthropicTextGenerator
import logging

logger = logging.getLogger("llmx")


def sanitize_provider(provider: str):
    if provider.lower() == "openai" or provider.lower() == "default" or provider.lower() == "azureopenai" or provider.lower() == "azureoai":
        return "openai"
    elif provider.lower() == "palm" or provider.lower() == "google":
        return "palm"
    elif provider.lower() == "cohere":
        return "cohere"
    elif provider.lower() == "hf" or provider.lower() == "huggingface":
        return "hf"
    elif provider.lower() == "anthropic" or provider.lower() == "claude":
        return "anthropic"
    else:
        raise ValueError(
            f"Invalid provider '{provider}'. Supported providers are 'openai', 'hf', 'palm', 'cohere', and 'anthropic'."
        )


def llm(provider: str = None, **kwargs):

    # load config. This will load the default config from
    # configs/config.default.yml if no path to a config file is specified. in
    # the environment variable LLMX_CONFIG_PATH
    config = load_config()
    if provider is None:
        # provider is not explicitly specified, use the default provider from the config file
        provider = config["model"]["provider"] if "provider" in config["model"] else None
        kwargs = config["model"]["parameters"] if "parameters" in config["model"] else {}
    if provider is None:
        logger.info("No provider specified. Defaulting to 'openai'.")
        provider = "openai"

    # sanitize provider
    provider = sanitize_provider(provider)

    # set the list of available models based on the config file
    models = config["providers"][provider]["models"] if "providers" in config and provider in config["providers"] else {}

    kwargs["provider"] = kwargs["provider"] if "provider" in kwargs else provider
    kwargs["models"] = kwargs["models"] if "models" in kwargs else models

    # print(kwargs)

    if provider.lower() == "openai":
        return OpenAITextGenerator(**kwargs)
    elif provider.lower() == "palm":
        return PalmTextGenerator(**kwargs)
    elif provider.lower() == "cohere":
        return CohereTextGenerator(**kwargs)
    elif provider.lower() == "anthropic":
        return AnthropicTextGenerator(**kwargs)
    elif provider.lower() == "hf":
        try:
            import transformers
        except ImportError:
            raise ImportError(
                "Please install the `transformers` package to use the HFTextGenerator class. pip install llmx[transformers]"
            )

        # Check if torch package is installed
        try:
            import torch
        except ImportError:
            raise ImportError(
                "Please install the `torch` package to use the HFTextGenerator class. pip install llmx[transformers]"
            )

        from .hf_textgen import HFTextGenerator

        return HFTextGenerator(**kwargs)

    else:
        raise ValueError(
            f"Invalid provider '{provider}'. Supported providers are 'openai', 'hf', 'palm', 'cohere', and 'anthropic'."
        )
